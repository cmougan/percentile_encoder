{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries and reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:33.548847Z",
     "start_time": "2020-05-23T15:23:33.526877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "utils.load_extension('collapsible_headings/main')\n",
       "utils.load_extension('hide_input/main')\n",
       "utils.load_extension('autosavetime/main')\n",
       "utils.load_extension('execute_time/ExecuteTime')\n",
       "utils.load_extension('code_prettify/code_prettify')\n",
       "utils.load_extension('scroll_down/main')\n",
       "utils.load_extension('jupyter-js-widgets/extension')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "utils.load_extension('collapsible_headings/main')\n",
    "utils.load_extension('hide_input/main')\n",
    "utils.load_extension('autosavetime/main')\n",
    "utils.load_extension('execute_time/ExecuteTime')\n",
    "utils.load_extension('code_prettify/code_prettify')\n",
    "utils.load_extension('scroll_down/main')\n",
    "utils.load_extension('jupyter-js-widgets/extension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:35.707109Z",
     "start_time": "2020-05-23T15:23:33.552538Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import pdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:35.718056Z",
     "start_time": "2020-05-23T15:23:35.708654Z"
    }
   },
   "outputs": [],
   "source": [
    "class TypeSelector(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Transformer that filters a type of columns of a given data frame.\n",
    "    '''\n",
    "    def __init__(self, dtype):\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        #print(\"Type Selector out shape {}\".format(X.select_dtypes(include=[self.dtype]).shape))\n",
    "        #print(X.select_dtypes(include=[self.dtype]).dtypes)\n",
    "        return X.select_dtypes(include=[self.dtype])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:04:00.826069Z",
     "start_time": "2020-05-23T15:04:00.800481Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:35.740725Z",
     "start_time": "2020-05-23T15:23:35.723902Z"
    }
   },
   "outputs": [],
   "source": [
    "# list of strings \n",
    "lst = ['Geeks', 'for', 'Geeks', 'for', 'AAA', 'for', 'Geeks'] \n",
    "  \n",
    "# list of int \n",
    "lst2 = [11, 22, 33, 44, 55, 66, 77] \n",
    "  \n",
    "# Calling DataFrame constructor after zipping \n",
    "# both lists, with columns specified \n",
    "df = pd.DataFrame(list(zip(lst, lst2)), \n",
    "               columns =['Company\\nLocation', 'Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:35.765956Z",
     "start_time": "2020-05-23T15:23:35.746145Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/flavors_of_cacao.csv')\n",
    "df = df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:35.777586Z",
     "start_time": "2020-05-23T15:23:35.769462Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'Rating')\n",
    "y = pd.DataFrame(df.Rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:35.799734Z",
     "start_time": "2020-05-23T15:23:35.779025Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company \\n(Maker-if known)</th>\n",
       "      <th>Specific Bean Origin\\nor Bar Name</th>\n",
       "      <th>REF</th>\n",
       "      <th>Review\\nDate</th>\n",
       "      <th>Cocoa\\nPercent</th>\n",
       "      <th>Company\\nLocation</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Bean\\nType</th>\n",
       "      <th>Broad Bean\\nOrigin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Agua Grande</td>\n",
       "      <td>1876</td>\n",
       "      <td>2016</td>\n",
       "      <td>63%</td>\n",
       "      <td>France</td>\n",
       "      <td>3.75</td>\n",
       "      <td></td>\n",
       "      <td>Sao Tome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Kpime</td>\n",
       "      <td>1676</td>\n",
       "      <td>2015</td>\n",
       "      <td>70%</td>\n",
       "      <td>France</td>\n",
       "      <td>2.75</td>\n",
       "      <td></td>\n",
       "      <td>Togo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company \\n(Maker-if known) Specific Bean Origin\\nor Bar Name   REF  \\\n",
       "0                   A. Morin                       Agua Grande  1876   \n",
       "1                   A. Morin                             Kpime  1676   \n",
       "\n",
       "   Review\\nDate Cocoa\\nPercent Company\\nLocation  Rating Bean\\nType  \\\n",
       "0          2016            63%            France    3.75              \n",
       "1          2015            70%            France    2.75              \n",
       "\n",
       "  Broad Bean\\nOrigin  \n",
       "0           Sao Tome  \n",
       "1               Togo  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:35.807350Z",
     "start_time": "2020-05-23T15:23:35.803198Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_enc = ['Company\\nLocation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Encoding Modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:36.065459Z",
     "start_time": "2020-05-23T15:23:35.809480Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Target Encoder\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "import category_encoders.utils as util\n",
    "\n",
    "__author__ = 'chappers'\n",
    "\n",
    "\n",
    "class PercentileEncoder(BaseEstimator, util.TransformerWithTargetMixin):\n",
    "    \"\"\"Target encoding for categorical features.\n",
    "\n",
    "    For the case of categorical target: features are replaced with a blend of posterior probability of the target\n",
    "    given particular categorical value and the prior probability of the target over all the training data.\n",
    "\n",
    "    For the case of continuous target: features are replaced with a blend of the expected value of the target\n",
    "    given particular categorical value and the expected value of the target over all the training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    verbose: int\n",
    "        integer indicating verbosity of the output. 0 for none.\n",
    "    cols: list\n",
    "        a list of columns to encode, if None, all string columns will be encoded.\n",
    "    drop_invariant: bool\n",
    "        boolean for whether or not to drop columns with 0 variance.\n",
    "    return_df: bool\n",
    "        boolean for whether to return a pandas DataFrame from transform (otherwise it will be a numpy array).\n",
    "    handle_missing: str\n",
    "        options are 'error', 'return_nan'  and 'value', defaults to 'value', which returns the target mean.\n",
    "    handle_unknown: str\n",
    "        options are 'error', 'return_nan' and 'value', defaults to 'value', which returns the target mean.\n",
    "    min_samples_leaf: int\n",
    "        minimum samples to take category average into account.\n",
    "    smoothing: float\n",
    "        smoothing effect to balance categorical average vs prior. Higher value means stronger regularization.\n",
    "        The value must be strictly bigger than 0.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> from category_encoders import *\n",
    "    >>> import pandas as pd\n",
    "    >>> from sklearn.datasets import load_boston\n",
    "    >>> bunch = load_boston()\n",
    "    >>> y = bunch.target\n",
    "    >>> X = pd.DataFrame(bunch.data, columns=bunch.feature_names)\n",
    "    >>> enc = TargetEncoder(cols=['CHAS', 'RAD']).fit(X, y)\n",
    "    >>> numeric_dataset = enc.transform(X)\n",
    "    >>> print(numeric_dataset.info())\n",
    "    <class 'pandas.core.frame.DataFrame'>\n",
    "    RangeIndex: 506 entries, 0 to 505\n",
    "    Data columns (total 13 columns):\n",
    "    CRIM       506 non-null float64\n",
    "    ZN         506 non-null float64\n",
    "    INDUS      506 non-null float64\n",
    "    CHAS       506 non-null float64\n",
    "    NOX        506 non-null float64\n",
    "    RM         506 non-null float64\n",
    "    AGE        506 non-null float64\n",
    "    DIS        506 non-null float64\n",
    "    RAD        506 non-null float64\n",
    "    TAX        506 non-null float64\n",
    "    PTRATIO    506 non-null float64\n",
    "    B          506 non-null float64\n",
    "    LSTAT      506 non-null float64\n",
    "    dtypes: float64(13)\n",
    "    memory usage: 51.5 KB\n",
    "    None\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    .. [1] A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems, from\n",
    "    https://dl.acm.org/citation.cfm?id=507538\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0, cols=None, drop_invariant=False, return_df=True, handle_missing='value',\n",
    "                     handle_unknown='value', min_samples_leaf=1, smoothing=1.0,percentile=50):\n",
    "        self.return_df = return_df\n",
    "        self.drop_invariant = drop_invariant\n",
    "        self.drop_cols = []\n",
    "        self.verbose = verbose\n",
    "        self.cols = cols\n",
    "        self.ordinal_encoder = None\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.smoothing = float(smoothing)  # Make smoothing a float so that python 2 does not treat as integer division\n",
    "        self._dim = None\n",
    "        self.mapping = None\n",
    "        self.handle_unknown = handle_unknown\n",
    "        self.handle_missing = handle_missing\n",
    "        self._mean = None\n",
    "        self.feature_names = None\n",
    "        self.percentile = percentile\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\"Fit encoder according to X and y.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : encoder\n",
    "            Returns self.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # unite the input into pandas types\n",
    "        X = util.convert_input(X)\n",
    "        y = util.convert_input_vector(y, X.index)\n",
    "\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"The length of X is \" + str(X.shape[0]) + \" but length of y is \" + str(y.shape[0]) + \".\")\n",
    "\n",
    "        self._dim = X.shape[1]\n",
    "\n",
    "        # if columns aren't passed, just use every string column\n",
    "        if self.cols is None:\n",
    "            self.cols = util.get_obj_cols(X)\n",
    "        else:\n",
    "            self.cols = util.convert_cols_to_list(self.cols)\n",
    "\n",
    "        if self.handle_missing == 'error':\n",
    "            if X[self.cols].isnull().any().any():\n",
    "                raise ValueError('Columns to be encoded can not contain null')\n",
    "\n",
    "        self.ordinal_encoder = OrdinalEncoder(\n",
    "            verbose=self.verbose,\n",
    "            cols=self.cols,\n",
    "            handle_unknown='value',\n",
    "            handle_missing='value'\n",
    "        )\n",
    "        self.ordinal_encoder = self.ordinal_encoder.fit(X)\n",
    "        X_ordinal = self.ordinal_encoder.transform(X)\n",
    "        self.mapping = self.fit_target_encoding(X_ordinal, y)\n",
    "        \n",
    "        X_temp = self.transform(X, override_return_df=True)\n",
    "        self.feature_names = list(X_temp.columns)\n",
    "\n",
    "        if self.drop_invariant:\n",
    "            self.drop_cols = []\n",
    "            X_temp = self.transform(X)\n",
    "            generated_cols = util.get_generated_cols(X, X_temp, self.cols)\n",
    "            self.drop_cols = [x for x in generated_cols if X_temp[x].var() <= 10e-5]\n",
    "            try:\n",
    "                [self.feature_names.remove(x) for x in self.drop_cols]\n",
    "            except KeyError as e:\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Could not remove column from feature names.\"\n",
    "                    \"Not found in generated cols.\\n{}\".format(e))\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def fit_target_encoding(self, X, y):\n",
    "        mapping = {}\n",
    "\n",
    "        for switch in self.ordinal_encoder.category_mapping:\n",
    "            col = switch.get('col')\n",
    "            values = switch.get('mapping')\n",
    "\n",
    "            prior = self._mean = np.percentile(y,self.percentile)\n",
    "\n",
    "            stats = y.groupby(X[col]).apply(lambda x: np.percentile(x,self.percentile))\n",
    "\n",
    "            \n",
    "            smoothing = stats\n",
    "            \n",
    "\n",
    "            if self.handle_unknown == 'return_nan':\n",
    "                smoothing.loc[-1] = np.nan\n",
    "            elif self.handle_unknown == 'value':\n",
    "                smoothing.loc[-1] = prior\n",
    "\n",
    "            if self.handle_missing == 'return_nan':\n",
    "                smoothing.loc[values.loc[np.nan]] = np.nan\n",
    "            elif self.handle_missing == 'value':\n",
    "                smoothing.loc[-2] = prior\n",
    "\n",
    "            mapping[col] = smoothing\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    def transform(self, X, y=None, override_return_df=False):\n",
    "        \"\"\"Perform the transformation to new categorical data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "        y : array-like, shape = [n_samples] when transform by leave one out\n",
    "            None, when transform without target info (such as transform test set)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        p : array, shape = [n_samples, n_numeric + N]\n",
    "            Transformed values with encoding applied.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if self.handle_missing == 'error':\n",
    "            if X[self.cols].isnull().any().any():\n",
    "                raise ValueError('Columns to be encoded can not contain null')\n",
    "\n",
    "        if self._dim is None:\n",
    "            raise ValueError('Must train encoder before it can be used to transform data.')\n",
    "\n",
    "        # unite the input into pandas types\n",
    "        X = util.convert_input(X)\n",
    "\n",
    "        # then make sure that it is the right size\n",
    "        if X.shape[1] != self._dim:\n",
    "            raise ValueError('Unexpected input dimension %d, expected %d' % (X.shape[1], self._dim,))\n",
    "\n",
    "        # if we are encoding the training data, we have to check the target\n",
    "        if y is not None:\n",
    "            y = util.convert_input_vector(y, X.index)\n",
    "            if X.shape[0] != y.shape[0]:\n",
    "                raise ValueError(\"The length of X is \" + str(X.shape[0]) + \" but length of y is \" + str(y.shape[0]) + \".\")\n",
    "\n",
    "        if not list(self.cols):\n",
    "            return X\n",
    "\n",
    "        X = self.ordinal_encoder.transform(X)\n",
    "\n",
    "        if self.handle_unknown == 'error':\n",
    "            if X[self.cols].isin([-1]).any().any():\n",
    "                raise ValueError('Unexpected categories found in dataframe')\n",
    "\n",
    "        X = self.target_encode(X)\n",
    "\n",
    "        if self.drop_invariant:\n",
    "            for col in self.drop_cols:\n",
    "                X.drop(col, 1, inplace=True)\n",
    "\n",
    "        if self.return_df or override_return_df:\n",
    "            return X\n",
    "        else:\n",
    "            return X.values\n",
    "\n",
    "\n",
    "    def target_encode(self, X_in):\n",
    "        X = X_in.copy(deep=True)\n",
    "\n",
    "        for col in self.cols:\n",
    "            X[col] = X[col].map(self.mapping[col])\n",
    "\n",
    "        return X\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"\n",
    "        Returns the names of all transformed / added columns.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        feature_names: list\n",
    "            A list with all feature names transformed or added.\n",
    "            Note: potentially dropped features are not included!\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(self.feature_names, list):\n",
    "            raise ValueError('Must fit data first. Affected feature names are not known before.')\n",
    "        else:\n",
    "            return self.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:36.084162Z",
     "start_time": "2020-05-23T15:23:36.076094Z"
    }
   },
   "outputs": [],
   "source": [
    "pe = PercentileEncoder(cols=cols_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:36.140242Z",
     "start_time": "2020-05-23T15:23:36.086493Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PercentileEncoder(cols=['Company\\nLocation'], drop_invariant=False,\n",
       "                  handle_missing='value', handle_unknown='value',\n",
       "                  min_samples_leaf=1, percentile=50, return_df=True,\n",
       "                  smoothing=1.0, verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:36.169048Z",
     "start_time": "2020-05-23T15:23:36.143115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company \\n(Maker-if known)</th>\n",
       "      <th>Specific Bean Origin\\nor Bar Name</th>\n",
       "      <th>REF</th>\n",
       "      <th>Review\\nDate</th>\n",
       "      <th>Cocoa\\nPercent</th>\n",
       "      <th>Company\\nLocation</th>\n",
       "      <th>Bean\\nType</th>\n",
       "      <th>Broad Bean\\nOrigin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Agua Grande</td>\n",
       "      <td>1876</td>\n",
       "      <td>2016</td>\n",
       "      <td>63%</td>\n",
       "      <td>3.25</td>\n",
       "      <td></td>\n",
       "      <td>Sao Tome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Kpime</td>\n",
       "      <td>1676</td>\n",
       "      <td>2015</td>\n",
       "      <td>70%</td>\n",
       "      <td>3.25</td>\n",
       "      <td></td>\n",
       "      <td>Togo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Atsane</td>\n",
       "      <td>1676</td>\n",
       "      <td>2015</td>\n",
       "      <td>70%</td>\n",
       "      <td>3.25</td>\n",
       "      <td></td>\n",
       "      <td>Togo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Akata</td>\n",
       "      <td>1680</td>\n",
       "      <td>2015</td>\n",
       "      <td>70%</td>\n",
       "      <td>3.25</td>\n",
       "      <td></td>\n",
       "      <td>Togo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Quilla</td>\n",
       "      <td>1704</td>\n",
       "      <td>2015</td>\n",
       "      <td>70%</td>\n",
       "      <td>3.25</td>\n",
       "      <td></td>\n",
       "      <td>Peru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>Zotter</td>\n",
       "      <td>Peru</td>\n",
       "      <td>647</td>\n",
       "      <td>2011</td>\n",
       "      <td>70%</td>\n",
       "      <td>3.25</td>\n",
       "      <td></td>\n",
       "      <td>Peru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>Zotter</td>\n",
       "      <td>Congo</td>\n",
       "      <td>749</td>\n",
       "      <td>2011</td>\n",
       "      <td>65%</td>\n",
       "      <td>3.25</td>\n",
       "      <td>Forastero</td>\n",
       "      <td>Congo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>Zotter</td>\n",
       "      <td>Kerala State</td>\n",
       "      <td>749</td>\n",
       "      <td>2011</td>\n",
       "      <td>65%</td>\n",
       "      <td>3.25</td>\n",
       "      <td>Forastero</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>Zotter</td>\n",
       "      <td>Kerala State</td>\n",
       "      <td>781</td>\n",
       "      <td>2011</td>\n",
       "      <td>62%</td>\n",
       "      <td>3.25</td>\n",
       "      <td></td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>Zotter</td>\n",
       "      <td>Brazil, Mitzi Blue</td>\n",
       "      <td>486</td>\n",
       "      <td>2010</td>\n",
       "      <td>65%</td>\n",
       "      <td>3.25</td>\n",
       "      <td></td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1795 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Company \\n(Maker-if known) Specific Bean Origin\\nor Bar Name   REF  \\\n",
       "0                      A. Morin                       Agua Grande  1876   \n",
       "1                      A. Morin                             Kpime  1676   \n",
       "2                      A. Morin                            Atsane  1676   \n",
       "3                      A. Morin                             Akata  1680   \n",
       "4                      A. Morin                            Quilla  1704   \n",
       "...                         ...                               ...   ...   \n",
       "1790                     Zotter                              Peru   647   \n",
       "1791                     Zotter                             Congo   749   \n",
       "1792                     Zotter                      Kerala State   749   \n",
       "1793                     Zotter                      Kerala State   781   \n",
       "1794                     Zotter                Brazil, Mitzi Blue   486   \n",
       "\n",
       "      Review\\nDate Cocoa\\nPercent  Company\\nLocation Bean\\nType  \\\n",
       "0             2016            63%               3.25              \n",
       "1             2015            70%               3.25              \n",
       "2             2015            70%               3.25              \n",
       "3             2015            70%               3.25              \n",
       "4             2015            70%               3.25              \n",
       "...            ...            ...                ...        ...   \n",
       "1790          2011            70%               3.25              \n",
       "1791          2011            65%               3.25  Forastero   \n",
       "1792          2011            65%               3.25  Forastero   \n",
       "1793          2011            62%               3.25              \n",
       "1794          2010            65%               3.25              \n",
       "\n",
       "     Broad Bean\\nOrigin  \n",
       "0              Sao Tome  \n",
       "1                  Togo  \n",
       "2                  Togo  \n",
       "3                  Togo  \n",
       "4                  Peru  \n",
       "...                 ...  \n",
       "1790               Peru  \n",
       "1791              Congo  \n",
       "1792              India  \n",
       "1793              India  \n",
       "1794             Brazil  \n",
       "\n",
       "[1795 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:36.223192Z",
     "start_time": "2020-05-23T15:23:36.173403Z"
    }
   },
   "outputs": [],
   "source": [
    "pe.fit_target_encoding(X[cols_enc],y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:36.277127Z",
     "start_time": "2020-05-23T15:23:36.225451Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company\\nLocation\n",
       "Amsterdam            3.500\n",
       "Argentina            3.500\n",
       "Australia            3.500\n",
       "Austria              3.250\n",
       "Belgium              3.250\n",
       "Bolivia              3.250\n",
       "Brazil               3.500\n",
       "Canada               3.250\n",
       "Chile                3.750\n",
       "Colombia             3.250\n",
       "Costa Rica           3.250\n",
       "Czech Republic       2.750\n",
       "Denmark              3.250\n",
       "Domincan Republic    3.250\n",
       "Ecuador              3.000\n",
       "Eucador              3.000\n",
       "Fiji                 3.375\n",
       "Finland              3.250\n",
       "France               3.250\n",
       "Germany              3.250\n",
       "Ghana                2.750\n",
       "Grenada              2.750\n",
       "Guatemala            3.250\n",
       "Honduras             3.250\n",
       "Hungary              3.250\n",
       "Iceland              3.500\n",
       "India                2.500\n",
       "Ireland              2.750\n",
       "Israel               3.500\n",
       "Italy                3.250\n",
       "Japan                3.000\n",
       "Lithuania            3.125\n",
       "Madagascar           3.250\n",
       "Martinique           2.750\n",
       "Mexico               2.875\n",
       "Netherlands          3.500\n",
       "New Zealand          3.250\n",
       "Niacragua            2.750\n",
       "Nicaragua            3.000\n",
       "Peru                 3.000\n",
       "Philippines          3.500\n",
       "Poland               3.500\n",
       "Portugal             2.750\n",
       "Puerto Rico          2.625\n",
       "Russia               3.250\n",
       "Sao Tome             3.375\n",
       "Scotland             3.500\n",
       "Singapore            3.500\n",
       "South Africa         2.500\n",
       "South Korea          3.250\n",
       "Spain                3.250\n",
       "St. Lucia            3.000\n",
       "Suriname             3.250\n",
       "Sweden               3.000\n",
       "Switzerland          3.250\n",
       "U.K.                 3.000\n",
       "U.S.A.               3.250\n",
       "Venezuela            3.250\n",
       "Vietnam              3.500\n",
       "Wales                2.750\n",
       "Name: Rating, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(cols_enc)['Rating'].apply(lambda x: np.percentile(x,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:36.303527Z",
     "start_time": "2020-05-23T15:23:36.294512Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tr,X_te,y_tr,y_te = train_test_split(df.drop(columns = 'Rating'),df.Rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:33:58.403971Z",
     "start_time": "2020-05-23T15:33:58.398894Z"
    }
   },
   "outputs": [],
   "source": [
    "from category_encoders.target_encoder import TargetEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:34:06.694650Z",
     "start_time": "2020-05-23T15:34:06.691064Z"
    }
   },
   "outputs": [],
   "source": [
    "pe = PercentileEncoder(cols=cols_enc,percentile=50)\n",
    "te = TargetEncoder(cols=cols_enc)\n",
    "lasso = linear_model.Lasso()\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:34:35.697176Z",
     "start_time": "2020-05-23T15:34:35.692690Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    \n",
    "    ('pe',pe),\n",
    "    ('selector', TypeSelector(np.number)), # Selects Numerical Columns only\n",
    "    ('scaler',scaler),\n",
    "    ('lasso',lasso)])\n",
    "\n",
    "pipeline_grid = {\n",
    "    \"lasso__alpha\":[0.000001,0.001,0.1,1],\n",
    "    \"pe__percentile\":[10,20,50,60,80,90]\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:34:35.955616Z",
     "start_time": "2020-05-23T15:34:35.951716Z"
    }
   },
   "outputs": [],
   "source": [
    "n_jobs=1\n",
    "cv = 3\n",
    "pipe_cv = GridSearchCV(pipe, param_grid=pipeline_grid, n_jobs = n_jobs, cv=cv,scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:34:39.037398Z",
     "start_time": "2020-05-23T15:34:36.135049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('pe',\n",
       "                                        PercentileEncoder(cols=['Company\\n'\n",
       "                                                                'Location'],\n",
       "                                                          drop_invariant=False,\n",
       "                                                          handle_missing='value',\n",
       "                                                          handle_unknown='value',\n",
       "                                                          min_samples_leaf=1,\n",
       "                                                          percentile=50,\n",
       "                                                          return_df=True,\n",
       "                                                          smoothing=1.0,\n",
       "                                                          verbose=0)),\n",
       "                                       ('selector',\n",
       "                                        TypeSelector(dtype=<class 'numpy.number'>)),\n",
       "                                       ('scaler',\n",
       "                                        Sta...\n",
       "                                              fit_intercept=True, max_iter=1000,\n",
       "                                              normalize=False, positive=False,\n",
       "                                              precompute=False,\n",
       "                                              random_state=None,\n",
       "                                              selection='cyclic', tol=0.0001,\n",
       "                                              warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=1,\n",
       "             param_grid={'lasso__alpha': [1e-06, 0.001, 0.1, 1],\n",
       "                         'pe__percentile': [10, 20, 50, 60, 80, 90]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_absolute_error', verbose=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_cv.fit(X_tr,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:34:39.072738Z",
     "start_time": "2020-05-23T15:34:39.039364Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_lasso__alpha</th>\n",
       "      <th>param_pe__percentile</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.032125</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>0.004436</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.001</td>\n",
       "      <td>80</td>\n",
       "      <td>{'lasso__alpha': 0.001, 'pe__percentile': 80}</td>\n",
       "      <td>-0.385864</td>\n",
       "      <td>-0.376629</td>\n",
       "      <td>-0.363013</td>\n",
       "      <td>-0.375178</td>\n",
       "      <td>0.009383</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.033773</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>0.004540</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>80</td>\n",
       "      <td>{'lasso__alpha': 1e-06, 'pe__percentile': 80}</td>\n",
       "      <td>-0.386268</td>\n",
       "      <td>-0.376596</td>\n",
       "      <td>-0.363359</td>\n",
       "      <td>-0.375417</td>\n",
       "      <td>0.009388</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.031709</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.004974</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20</td>\n",
       "      <td>{'lasso__alpha': 0.001, 'pe__percentile': 20}</td>\n",
       "      <td>-0.384828</td>\n",
       "      <td>-0.379161</td>\n",
       "      <td>-0.365190</td>\n",
       "      <td>-0.376401</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.045761</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.004918</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>20</td>\n",
       "      <td>{'lasso__alpha': 1e-06, 'pe__percentile': 20}</td>\n",
       "      <td>-0.385550</td>\n",
       "      <td>-0.379219</td>\n",
       "      <td>-0.365507</td>\n",
       "      <td>-0.376767</td>\n",
       "      <td>0.008363</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.034104</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.004454</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.001</td>\n",
       "      <td>90</td>\n",
       "      <td>{'lasso__alpha': 0.001, 'pe__percentile': 90}</td>\n",
       "      <td>-0.387796</td>\n",
       "      <td>-0.378444</td>\n",
       "      <td>-0.364329</td>\n",
       "      <td>-0.376866</td>\n",
       "      <td>0.009643</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.037138</td>\n",
       "      <td>0.005972</td>\n",
       "      <td>0.004889</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.001</td>\n",
       "      <td>60</td>\n",
       "      <td>{'lasso__alpha': 0.001, 'pe__percentile': 60}</td>\n",
       "      <td>-0.384781</td>\n",
       "      <td>-0.378483</td>\n",
       "      <td>-0.367599</td>\n",
       "      <td>-0.376961</td>\n",
       "      <td>0.007095</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.043513</td>\n",
       "      <td>0.005532</td>\n",
       "      <td>0.007118</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>60</td>\n",
       "      <td>{'lasso__alpha': 1e-06, 'pe__percentile': 60}</td>\n",
       "      <td>-0.385107</td>\n",
       "      <td>-0.378497</td>\n",
       "      <td>-0.368040</td>\n",
       "      <td>-0.377222</td>\n",
       "      <td>0.007024</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.031443</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.004983</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>90</td>\n",
       "      <td>{'lasso__alpha': 1e-06, 'pe__percentile': 90}</td>\n",
       "      <td>-0.389156</td>\n",
       "      <td>-0.378431</td>\n",
       "      <td>-0.364465</td>\n",
       "      <td>-0.377360</td>\n",
       "      <td>0.010107</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.029476</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.1</td>\n",
       "      <td>60</td>\n",
       "      <td>{'lasso__alpha': 0.1, 'pe__percentile': 60}</td>\n",
       "      <td>-0.386283</td>\n",
       "      <td>-0.380763</td>\n",
       "      <td>-0.365381</td>\n",
       "      <td>-0.377485</td>\n",
       "      <td>0.008841</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.034582</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50</td>\n",
       "      <td>{'lasso__alpha': 0.001, 'pe__percentile': 50}</td>\n",
       "      <td>-0.390033</td>\n",
       "      <td>-0.378903</td>\n",
       "      <td>-0.364060</td>\n",
       "      <td>-0.377675</td>\n",
       "      <td>0.010637</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.029120</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.004279</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>{'lasso__alpha': 0.1, 'pe__percentile': 50}</td>\n",
       "      <td>-0.385891</td>\n",
       "      <td>-0.381741</td>\n",
       "      <td>-0.365987</td>\n",
       "      <td>-0.377882</td>\n",
       "      <td>0.008571</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.028863</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.004670</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>{'lasso__alpha': 0.1, 'pe__percentile': 20}</td>\n",
       "      <td>-0.386212</td>\n",
       "      <td>-0.381789</td>\n",
       "      <td>-0.366330</td>\n",
       "      <td>-0.378119</td>\n",
       "      <td>0.008520</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.039077</td>\n",
       "      <td>0.006563</td>\n",
       "      <td>0.005061</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>50</td>\n",
       "      <td>{'lasso__alpha': 1e-06, 'pe__percentile': 50}</td>\n",
       "      <td>-0.391097</td>\n",
       "      <td>-0.379035</td>\n",
       "      <td>-0.364472</td>\n",
       "      <td>-0.378211</td>\n",
       "      <td>0.010883</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.028993</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>{'lasso__alpha': 0.1, 'pe__percentile': 80}</td>\n",
       "      <td>-0.386587</td>\n",
       "      <td>-0.381747</td>\n",
       "      <td>-0.367423</td>\n",
       "      <td>-0.378594</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.030433</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.004470</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>{'lasso__alpha': 0.001, 'pe__percentile': 10}</td>\n",
       "      <td>-0.385465</td>\n",
       "      <td>-0.382350</td>\n",
       "      <td>-0.367948</td>\n",
       "      <td>-0.378595</td>\n",
       "      <td>0.007627</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.029057</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>{'lasso__alpha': 1, 'pe__percentile': 60}</td>\n",
       "      <td>-0.386757</td>\n",
       "      <td>-0.381789</td>\n",
       "      <td>-0.368290</td>\n",
       "      <td>-0.378953</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.029335</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.004596</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>{'lasso__alpha': 1, 'pe__percentile': 50}</td>\n",
       "      <td>-0.386757</td>\n",
       "      <td>-0.381789</td>\n",
       "      <td>-0.368290</td>\n",
       "      <td>-0.378953</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.028688</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.004402</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>{'lasso__alpha': 1, 'pe__percentile': 20}</td>\n",
       "      <td>-0.386757</td>\n",
       "      <td>-0.381789</td>\n",
       "      <td>-0.368290</td>\n",
       "      <td>-0.378953</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.031326</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>{'lasso__alpha': 1, 'pe__percentile': 90}</td>\n",
       "      <td>-0.386757</td>\n",
       "      <td>-0.381789</td>\n",
       "      <td>-0.368290</td>\n",
       "      <td>-0.378953</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.029237</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.004368</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.1</td>\n",
       "      <td>90</td>\n",
       "      <td>{'lasso__alpha': 0.1, 'pe__percentile': 90}</td>\n",
       "      <td>-0.386757</td>\n",
       "      <td>-0.381789</td>\n",
       "      <td>-0.368290</td>\n",
       "      <td>-0.378953</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.031592</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.004843</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>{'lasso__alpha': 0.1, 'pe__percentile': 10}</td>\n",
       "      <td>-0.386757</td>\n",
       "      <td>-0.381789</td>\n",
       "      <td>-0.368290</td>\n",
       "      <td>-0.378953</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.029241</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.005266</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>{'lasso__alpha': 1, 'pe__percentile': 80}</td>\n",
       "      <td>-0.386757</td>\n",
       "      <td>-0.381789</td>\n",
       "      <td>-0.368290</td>\n",
       "      <td>-0.378953</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.028681</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.004593</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>{'lasso__alpha': 1, 'pe__percentile': 10}</td>\n",
       "      <td>-0.386757</td>\n",
       "      <td>-0.381789</td>\n",
       "      <td>-0.368290</td>\n",
       "      <td>-0.378953</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.055312</td>\n",
       "      <td>0.012420</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>0.003239</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>10</td>\n",
       "      <td>{'lasso__alpha': 1e-06, 'pe__percentile': 10}</td>\n",
       "      <td>-0.386418</td>\n",
       "      <td>-0.382530</td>\n",
       "      <td>-0.368111</td>\n",
       "      <td>-0.379028</td>\n",
       "      <td>0.007872</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "10       0.032125      0.001955         0.004436        0.000383   \n",
       "4        0.033773      0.001356         0.004540        0.000195   \n",
       "7        0.031709      0.002070         0.004974        0.000355   \n",
       "1        0.045761      0.003490         0.007949        0.004918   \n",
       "11       0.034104      0.000579         0.004454        0.000124   \n",
       "9        0.037138      0.005972         0.004889        0.000529   \n",
       "3        0.043513      0.005532         0.007118        0.000802   \n",
       "5        0.031443      0.000922         0.004983        0.000406   \n",
       "15       0.029476      0.000860         0.004394        0.000108   \n",
       "8        0.034582      0.000679         0.005274        0.000659   \n",
       "14       0.029120      0.000320         0.004279        0.000123   \n",
       "13       0.028863      0.000054         0.004670        0.000402   \n",
       "2        0.039077      0.006563         0.005061        0.000514   \n",
       "16       0.028993      0.000348         0.004386        0.000128   \n",
       "6        0.030433      0.000654         0.004470        0.000275   \n",
       "21       0.029057      0.000168         0.004372        0.000041   \n",
       "20       0.029335      0.001231         0.004596        0.000177   \n",
       "19       0.028688      0.000134         0.004402        0.000076   \n",
       "23       0.031326      0.003268         0.004713        0.000477   \n",
       "17       0.029237      0.000625         0.004368        0.000118   \n",
       "12       0.031592      0.001432         0.004843        0.000451   \n",
       "22       0.029241      0.000628         0.005266        0.001496   \n",
       "18       0.028681      0.000238         0.004593        0.000442   \n",
       "0        0.055312      0.012420         0.009112        0.003239   \n",
       "\n",
       "   param_lasso__alpha param_pe__percentile  \\\n",
       "10              0.001                   80   \n",
       "4               1e-06                   80   \n",
       "7               0.001                   20   \n",
       "1               1e-06                   20   \n",
       "11              0.001                   90   \n",
       "9               0.001                   60   \n",
       "3               1e-06                   60   \n",
       "5               1e-06                   90   \n",
       "15                0.1                   60   \n",
       "8               0.001                   50   \n",
       "14                0.1                   50   \n",
       "13                0.1                   20   \n",
       "2               1e-06                   50   \n",
       "16                0.1                   80   \n",
       "6               0.001                   10   \n",
       "21                  1                   60   \n",
       "20                  1                   50   \n",
       "19                  1                   20   \n",
       "23                  1                   90   \n",
       "17                0.1                   90   \n",
       "12                0.1                   10   \n",
       "22                  1                   80   \n",
       "18                  1                   10   \n",
       "0               1e-06                   10   \n",
       "\n",
       "                                           params  split0_test_score  \\\n",
       "10  {'lasso__alpha': 0.001, 'pe__percentile': 80}          -0.385864   \n",
       "4   {'lasso__alpha': 1e-06, 'pe__percentile': 80}          -0.386268   \n",
       "7   {'lasso__alpha': 0.001, 'pe__percentile': 20}          -0.384828   \n",
       "1   {'lasso__alpha': 1e-06, 'pe__percentile': 20}          -0.385550   \n",
       "11  {'lasso__alpha': 0.001, 'pe__percentile': 90}          -0.387796   \n",
       "9   {'lasso__alpha': 0.001, 'pe__percentile': 60}          -0.384781   \n",
       "3   {'lasso__alpha': 1e-06, 'pe__percentile': 60}          -0.385107   \n",
       "5   {'lasso__alpha': 1e-06, 'pe__percentile': 90}          -0.389156   \n",
       "15    {'lasso__alpha': 0.1, 'pe__percentile': 60}          -0.386283   \n",
       "8   {'lasso__alpha': 0.001, 'pe__percentile': 50}          -0.390033   \n",
       "14    {'lasso__alpha': 0.1, 'pe__percentile': 50}          -0.385891   \n",
       "13    {'lasso__alpha': 0.1, 'pe__percentile': 20}          -0.386212   \n",
       "2   {'lasso__alpha': 1e-06, 'pe__percentile': 50}          -0.391097   \n",
       "16    {'lasso__alpha': 0.1, 'pe__percentile': 80}          -0.386587   \n",
       "6   {'lasso__alpha': 0.001, 'pe__percentile': 10}          -0.385465   \n",
       "21      {'lasso__alpha': 1, 'pe__percentile': 60}          -0.386757   \n",
       "20      {'lasso__alpha': 1, 'pe__percentile': 50}          -0.386757   \n",
       "19      {'lasso__alpha': 1, 'pe__percentile': 20}          -0.386757   \n",
       "23      {'lasso__alpha': 1, 'pe__percentile': 90}          -0.386757   \n",
       "17    {'lasso__alpha': 0.1, 'pe__percentile': 90}          -0.386757   \n",
       "12    {'lasso__alpha': 0.1, 'pe__percentile': 10}          -0.386757   \n",
       "22      {'lasso__alpha': 1, 'pe__percentile': 80}          -0.386757   \n",
       "18      {'lasso__alpha': 1, 'pe__percentile': 10}          -0.386757   \n",
       "0   {'lasso__alpha': 1e-06, 'pe__percentile': 10}          -0.386418   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "10          -0.376629          -0.363013        -0.375178        0.009383   \n",
       "4           -0.376596          -0.363359        -0.375417        0.009388   \n",
       "7           -0.379161          -0.365190        -0.376401        0.008250   \n",
       "1           -0.379219          -0.365507        -0.376767        0.008363   \n",
       "11          -0.378444          -0.364329        -0.376866        0.009643   \n",
       "9           -0.378483          -0.367599        -0.376961        0.007095   \n",
       "3           -0.378497          -0.368040        -0.377222        0.007024   \n",
       "5           -0.378431          -0.364465        -0.377360        0.010107   \n",
       "15          -0.380763          -0.365381        -0.377485        0.008841   \n",
       "8           -0.378903          -0.364060        -0.377675        0.010637   \n",
       "14          -0.381741          -0.365987        -0.377882        0.008571   \n",
       "13          -0.381789          -0.366330        -0.378119        0.008520   \n",
       "2           -0.379035          -0.364472        -0.378211        0.010883   \n",
       "16          -0.381747          -0.367423        -0.378594        0.008134   \n",
       "6           -0.382350          -0.367948        -0.378595        0.007627   \n",
       "21          -0.381789          -0.368290        -0.378953        0.007800   \n",
       "20          -0.381789          -0.368290        -0.378953        0.007800   \n",
       "19          -0.381789          -0.368290        -0.378953        0.007800   \n",
       "23          -0.381789          -0.368290        -0.378953        0.007800   \n",
       "17          -0.381789          -0.368290        -0.378953        0.007800   \n",
       "12          -0.381789          -0.368290        -0.378953        0.007800   \n",
       "22          -0.381789          -0.368290        -0.378953        0.007800   \n",
       "18          -0.381789          -0.368290        -0.378953        0.007800   \n",
       "0           -0.382530          -0.368111        -0.379028        0.007872   \n",
       "\n",
       "    rank_test_score  \n",
       "10                1  \n",
       "4                 2  \n",
       "7                 3  \n",
       "1                 4  \n",
       "11                5  \n",
       "9                 6  \n",
       "3                 7  \n",
       "5                 8  \n",
       "15                9  \n",
       "8                10  \n",
       "14               11  \n",
       "13               12  \n",
       "2                13  \n",
       "16               14  \n",
       "6                15  \n",
       "21               16  \n",
       "20               16  \n",
       "19               16  \n",
       "23               16  \n",
       "17               16  \n",
       "12               16  \n",
       "22               16  \n",
       "18               16  \n",
       "0                24  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pipe_cv.cv_results_).sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:37.949935Z",
     "start_time": "2020-05-23T15:23:37.934350Z"
    }
   },
   "outputs": [],
   "source": [
    "import category_encoders.utils as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:37.957243Z",
     "start_time": "2020-05-23T15:23:37.952737Z"
    }
   },
   "outputs": [],
   "source": [
    "X = util.convert_input(X)\n",
    "y = util.convert_input_vector(y, X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:38.182045Z",
     "start_time": "2020-05-23T15:23:37.961698Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grouper for '<class 'pandas.core.frame.DataFrame'>' not 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b74e77db2685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_enc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed)\u001b[0m\n\u001b[1;32m   1685\u001b[0m             \u001b[0mgroup_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m             \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1687\u001b[0;31m             \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1688\u001b[0m         )\n\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                 \u001b[0mmutated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmutated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m             )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, index, grouper, obj, name, level, sort, observed, in_axis)\u001b[0m\n\u001b[1;32m    343\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ndim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Grouper for '{t}' not 1-dimensional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 if not (\n",
      "\u001b[0;31mValueError\u001b[0m: Grouper for '<class 'pandas.core.frame.DataFrame'>' not 1-dimensional"
     ]
    }
   ],
   "source": [
    "y.groupby(X[cols_enc]).agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:38.184638Z",
     "start_time": "2020-05-23T15:23:33.562Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(cols_enc)['Rating'].apply(lambda x: np.percentile(x,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:38.186331Z",
     "start_time": "2020-05-23T15:23:33.563Z"
    }
   },
   "outputs": [],
   "source": [
    "a = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:38.191106Z",
     "start_time": "2020-05-23T15:23:33.564Z"
    }
   },
   "outputs": [],
   "source": [
    "np.percentile(a,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:38.195600Z",
     "start_time": "2020-05-23T15:23:33.566Z"
    }
   },
   "outputs": [],
   "source": [
    "class Percentile_Encoding(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    This class implements fit and transform methods that allows to encode categorical features in different ways.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,columns=\"All\",return_categorical=True,percentile= 50):\n",
    "        #cols: list -> a list of columns to encode, if All, all string columns will be encoded.\n",
    "        \n",
    "        #self._allowed_encodings = [\"TargetEncoder\",\"WOEEncoder\",\"CatBoostEncoder\",\"OneHotEncoder\"]           \n",
    "        #assert encoding_type in self._allowed_encodings, \"the encoding type introduced {} is not valid. Please use one in {}\".format(encoding_type, self._allowed_encodings)\n",
    "        #self.encoding_type = encoding_type\n",
    "        \n",
    "        self.columns = columns\n",
    "        self.return_categorical = return_categorical\n",
    "        self.percentile = percentile\n",
    "        \n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \"\"\"\n",
    "        This method learns encodings for categorical variables/values.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Obtain a list of categorical variables\n",
    "        if self.columns == \"All\":\n",
    "            self.categorical_cols = X.columns[X.dtypes==object].tolist() +  X.columns[X.dtypes==\"category\"].tolist()\n",
    "        else:\n",
    "            self.categorical_cols = self.columns\n",
    "        \n",
    "    \n",
    "        # Split the data into categorical and numerical\n",
    "        self.data_encode = X[self.categorical_cols]\n",
    "\n",
    "        # Fit the encoder\n",
    "        self.mapping = self.fit_percentil_encoding(self.data_encode, y)\n",
    "        return self\n",
    "    \n",
    "    def fit_percentil_encoding(self, X, y):\n",
    "        mapping = {}\n",
    "\n",
    "        for col in X.columns:\n",
    "            \n",
    "            stats = X.groupby(col)[y.Rating].apply(lambda x: np.percentile(x,self.percentile))\n",
    "            \n",
    "            mapping[col] = stats\n",
    "        return mapping\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        \n",
    "        if self.columns == \"All\":\n",
    "            self.categorical_cols = X.columns[X.dtypes==object].tolist() +  X.columns[X.dtypes==\"category\"].tolist()\n",
    "        else:\n",
    "            self.categorical_cols = self.columns\n",
    "        \n",
    "       \n",
    "    \n",
    "        # Split the data into categorical and numerical\n",
    "        self.data_encode = X[self.categorical_cols]\n",
    "        \n",
    "        # Transform the data\n",
    "        self.transformed = self.enc.transform(self.data_encode)\n",
    "        \n",
    "        # Modify the names of the columns with the proper suffix\n",
    "        self.new_names = []\n",
    "        for c in self.transformed.columns:\n",
    "            self.new_names.append(c+'_'+self.encoding_type)\n",
    "        self.transformed.columns = self.new_names\n",
    "         \n",
    "        if self.return_categorical:\n",
    "            #print('The encoding {} has made {} columns, the input was {} and the output shape{}'.\n",
    "             #     format(self.encoding_type,self.transformed.shape, X.shape,self.transformed.join(X).shape))\n",
    "            #print(self.transformed.join(X).dtypes)\n",
    "\n",
    "            return self.transformed.join(X)\n",
    "        else:\n",
    "            return self.transformed.join(X)._get_numeric_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:38.199873Z",
     "start_time": "2020-05-23T15:23:33.567Z"
    }
   },
   "outputs": [],
   "source": [
    "pe = Percentile_Encoding(columns=cols_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:38.203108Z",
     "start_time": "2020-05-23T15:23:33.569Z"
    }
   },
   "outputs": [],
   "source": [
    "pe.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:38.211013Z",
     "start_time": "2020-05-23T15:23:33.571Z"
    }
   },
   "outputs": [],
   "source": [
    "y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Target Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T11:27:51.632198Z",
     "start_time": "2020-05-23T11:27:51.429188Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:38.212050Z",
     "start_time": "2020-05-23T15:23:33.587Z"
    }
   },
   "outputs": [],
   "source": [
    "te = TargetEncoder(cols=cols_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:38.217776Z",
     "start_time": "2020-05-23T15:23:33.591Z"
    }
   },
   "outputs": [],
   "source": [
    "te.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:38.222024Z",
     "start_time": "2020-05-23T15:23:33.593Z"
    }
   },
   "outputs": [],
   "source": [
    "te.fit_target_encoding(X[cols_enc],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:38.224762Z",
     "start_time": "2020-05-23T15:23:33.595Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(cols_enc)['Rating'].apply(lambda x: np.percentile(x,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Target Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:38.227016Z",
     "start_time": "2020-05-23T15:23:33.601Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Target Encoder\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "import category_encoders.utils as util\n",
    "\n",
    "__author__ = 'chappers'\n",
    "\n",
    "\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Target encoding for categorical features.\n",
    "\n",
    "    For the case of categorical target: features are replaced with a blend of posterior probability of the target\n",
    "    given particular categorical value and the prior probability of the target over all the training data.\n",
    "\n",
    "    For the case of continuous target: features are replaced with a blend of the expected value of the target\n",
    "    given particular categorical value and the expected value of the target over all the training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    verbose: int\n",
    "        integer indicating verbosity of the output. 0 for none.\n",
    "    cols: list\n",
    "        a list of columns to encode, if None, all string columns will be encoded.\n",
    "    drop_invariant: bool\n",
    "        boolean for whether or not to drop columns with 0 variance.\n",
    "    return_df: bool\n",
    "        boolean for whether to return a pandas DataFrame from transform (otherwise it will be a numpy array).\n",
    "    handle_missing: str\n",
    "        options are 'error', 'return_nan'  and 'value', defaults to 'value', which returns the target mean.\n",
    "    handle_unknown: str\n",
    "        options are 'error', 'return_nan' and 'value', defaults to 'value', which returns the target mean.\n",
    "    min_samples_leaf: int\n",
    "        minimum samples to take category average into account.\n",
    "    smoothing: float\n",
    "        smoothing effect to balance categorical average vs prior. Higher value means stronger regularization.\n",
    "        The value must be strictly bigger than 0.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> from category_encoders import *\n",
    "    >>> import pandas as pd\n",
    "    >>> from sklearn.datasets import load_boston\n",
    "    >>> bunch = load_boston()\n",
    "    >>> y = bunch.target\n",
    "    >>> X = pd.DataFrame(bunch.data, columns=bunch.feature_names)\n",
    "    >>> enc = TargetEncoder(cols=['CHAS', 'RAD']).fit(X, y)\n",
    "    >>> numeric_dataset = enc.transform(X)\n",
    "    >>> print(numeric_dataset.info())\n",
    "    <class 'pandas.core.frame.DataFrame'>\n",
    "    RangeIndex: 506 entries, 0 to 505\n",
    "    Data columns (total 13 columns):\n",
    "    CRIM       506 non-null float64\n",
    "    ZN         506 non-null float64\n",
    "    INDUS      506 non-null float64\n",
    "    CHAS       506 non-null float64\n",
    "    NOX        506 non-null float64\n",
    "    RM         506 non-null float64\n",
    "    AGE        506 non-null float64\n",
    "    DIS        506 non-null float64\n",
    "    RAD        506 non-null float64\n",
    "    TAX        506 non-null float64\n",
    "    PTRATIO    506 non-null float64\n",
    "    B          506 non-null float64\n",
    "    LSTAT      506 non-null float64\n",
    "    dtypes: float64(13)\n",
    "    memory usage: 51.5 KB\n",
    "    None\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    .. [1] A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems, from\n",
    "    https://dl.acm.org/citation.cfm?id=507538\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0, cols=None, drop_invariant=False, return_df=True, handle_missing='value',\n",
    "                     handle_unknown='value', min_samples_leaf=1, smoothing=1.0):\n",
    "        self.return_df = return_df\n",
    "        self.drop_invariant = drop_invariant\n",
    "        self.drop_cols = []\n",
    "        self.verbose = verbose\n",
    "        self.cols = cols\n",
    "        self.ordinal_encoder = None\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.smoothing = float(smoothing)  # Make smoothing a float so that python 2 does not treat as integer division\n",
    "        self._dim = None\n",
    "        self.mapping = None\n",
    "        self.handle_unknown = handle_unknown\n",
    "        self.handle_missing = handle_missing\n",
    "        self._mean = None\n",
    "        self.feature_names = None\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\"Fit encoder according to X and y.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : encoder\n",
    "            Returns self.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # unite the input into pandas types\n",
    "        X = util.convert_input(X)\n",
    "        y = util.convert_input_vector(y, X.index)\n",
    "\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"The length of X is \" + str(X.shape[0]) + \" but length of y is \" + str(y.shape[0]) + \".\")\n",
    "\n",
    "        self._dim = X.shape[1]\n",
    "\n",
    "        # if columns aren't passed, just use every string column\n",
    "        if self.cols is None:\n",
    "            self.cols = util.get_obj_cols(X)\n",
    "        else:\n",
    "            self.cols = util.convert_cols_to_list(self.cols)\n",
    "\n",
    "        if self.handle_missing == 'error':\n",
    "            if X[self.cols].isnull().any().any():\n",
    "                raise ValueError('Columns to be encoded can not contain null')\n",
    "\n",
    "        self.ordinal_encoder = OrdinalEncoder(\n",
    "            verbose=self.verbose,\n",
    "            cols=self.cols,\n",
    "            handle_unknown='value',\n",
    "            handle_missing='value'\n",
    "        )\n",
    "        self.ordinal_encoder = self.ordinal_encoder.fit(X)\n",
    "        X_ordinal = self.ordinal_encoder.transform(X)\n",
    "        self.mapping = self.fit_target_encoding(X_ordinal, y)\n",
    "        \n",
    "        X_temp = self.transform(X, override_return_df=True)\n",
    "        self.feature_names = list(X_temp.columns)\n",
    "\n",
    "        if self.drop_invariant:\n",
    "            self.drop_cols = []\n",
    "            X_temp = self.transform(X)\n",
    "            generated_cols = util.get_generated_cols(X, X_temp, self.cols)\n",
    "            self.drop_cols = [x for x in generated_cols if X_temp[x].var() <= 10e-5]\n",
    "            try:\n",
    "                [self.feature_names.remove(x) for x in self.drop_cols]\n",
    "            except KeyError as e:\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Could not remove column from feature names.\"\n",
    "                    \"Not found in generated cols.\\n{}\".format(e))\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def fit_target_encoding(self, X, y):\n",
    "        mapping = {}\n",
    "\n",
    "        for switch in self.ordinal_encoder.category_mapping:\n",
    "            col = switch.get('col')\n",
    "            values = switch.get('mapping')\n",
    "\n",
    "            prior = self._mean = y.mean()\n",
    "\n",
    "            stats = y.groupby(X[col]).agg(['count', 'mean'])\n",
    "\n",
    "            smoove = 1 / (1 + np.exp(-(stats['count'] - self.min_samples_leaf) / self.smoothing))\n",
    "            smoothing = prior * (1 - smoove) + stats['mean'] * smoove\n",
    "            \n",
    "            smoothing\n",
    "            smoothing[stats['count'] == 1] = prior\n",
    "            \n",
    "\n",
    "            if self.handle_unknown == 'return_nan':\n",
    "                smoothing.loc[-1] = np.nan\n",
    "            elif self.handle_unknown == 'value':\n",
    "                smoothing.loc[-1] = prior\n",
    "\n",
    "            if self.handle_missing == 'return_nan':\n",
    "                smoothing.loc[values.loc[np.nan]] = np.nan\n",
    "            elif self.handle_missing == 'value':\n",
    "                smoothing.loc[-2] = prior\n",
    "\n",
    "            mapping[col] = smoothing\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    def transform(self, X, y=None, override_return_df=False):\n",
    "        \"\"\"Perform the transformation to new categorical data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "        y : array-like, shape = [n_samples] when transform by leave one out\n",
    "            None, when transform without target info (such as transform test set)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        p : array, shape = [n_samples, n_numeric + N]\n",
    "            Transformed values with encoding applied.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if self.handle_missing == 'error':\n",
    "            if X[self.cols].isnull().any().any():\n",
    "                raise ValueError('Columns to be encoded can not contain null')\n",
    "\n",
    "        if self._dim is None:\n",
    "            raise ValueError('Must train encoder before it can be used to transform data.')\n",
    "\n",
    "        # unite the input into pandas types\n",
    "        X = util.convert_input(X)\n",
    "\n",
    "        # then make sure that it is the right size\n",
    "        if X.shape[1] != self._dim:\n",
    "            raise ValueError('Unexpected input dimension %d, expected %d' % (X.shape[1], self._dim,))\n",
    "\n",
    "        # if we are encoding the training data, we have to check the target\n",
    "        if y is not None:\n",
    "            y = util.convert_input_vector(y, X.index)\n",
    "            if X.shape[0] != y.shape[0]:\n",
    "                raise ValueError(\"The length of X is \" + str(X.shape[0]) + \" but length of y is \" + str(y.shape[0]) + \".\")\n",
    "\n",
    "        if not list(self.cols):\n",
    "            return X\n",
    "\n",
    "        X = self.ordinal_encoder.transform(X)\n",
    "\n",
    "        if self.handle_unknown == 'error':\n",
    "            if X[self.cols].isin([-1]).any().any():\n",
    "                raise ValueError('Unexpected categories found in dataframe')\n",
    "\n",
    "        X = self.target_encode(X)\n",
    "\n",
    "        if self.drop_invariant:\n",
    "            for col in self.drop_cols:\n",
    "                X.drop(col, 1, inplace=True)\n",
    "\n",
    "        if self.return_df or override_return_df:\n",
    "            return X\n",
    "        else:\n",
    "            return X.values\n",
    "\n",
    "\n",
    "    def target_encode(self, X_in):\n",
    "        X = X_in.copy(deep=True)\n",
    "\n",
    "        for col in self.cols:\n",
    "            X[col] = X[col].map(self.mapping[col])\n",
    "\n",
    "        return X\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"\n",
    "        Returns the names of all transformed / added columns.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        feature_names: list\n",
    "            A list with all feature names transformed or added.\n",
    "            Note: potentially dropped features are not included!\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(self.feature_names, list):\n",
    "            raise ValueError('Must fit data first. Affected feature names are not known before.')\n",
    "        else:\n",
    "            return self.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:38.228476Z",
     "start_time": "2020-05-23T15:23:33.603Z"
    }
   },
   "outputs": [],
   "source": [
    "te = TargetEncoder(cols=cols_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:38.229605Z",
     "start_time": "2020-05-23T15:23:33.605Z"
    }
   },
   "outputs": [],
   "source": [
    "te.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T15:23:38.241981Z",
     "start_time": "2020-05-23T15:23:33.607Z"
    }
   },
   "outputs": [],
   "source": [
    "te.transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
