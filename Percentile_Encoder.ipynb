{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries and reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.772983Z",
     "start_time": "2020-05-23T12:08:15.758363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "utils.load_extension('collapsible_headings/main')\n",
       "utils.load_extension('hide_input/main')\n",
       "utils.load_extension('autosavetime/main')\n",
       "utils.load_extension('execute_time/ExecuteTime')\n",
       "utils.load_extension('code_prettify/code_prettify')\n",
       "utils.load_extension('scroll_down/main')\n",
       "utils.load_extension('jupyter-js-widgets/extension')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "utils.load_extension('collapsible_headings/main')\n",
    "utils.load_extension('hide_input/main')\n",
    "utils.load_extension('autosavetime/main')\n",
    "utils.load_extension('execute_time/ExecuteTime')\n",
    "utils.load_extension('code_prettify/code_prettify')\n",
    "utils.load_extension('scroll_down/main')\n",
    "utils.load_extension('jupyter-js-widgets/extension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:32.343071Z",
     "start_time": "2020-05-23T12:08:32.337901Z"
    }
   },
   "outputs": [],
   "source": [
    "class TypeSelector(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Transformer that filters a type of columns of a given data frame.\n",
    "    '''\n",
    "    def __init__(self, dtype):\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        #print(\"Type Selector out shape {}\".format(X.select_dtypes(include=[self.dtype]).shape))\n",
    "        #print(X.select_dtypes(include=[self.dtype]).dtypes)\n",
    "        return X.select_dtypes(include=[self.dtype])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:31.612724Z",
     "start_time": "2020-05-23T12:08:30.581505Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import pdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:33.913456Z",
     "start_time": "2020-05-23T12:08:33.891237Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/flavors_of_cacao.csv')\n",
    "df = df.fillna(0)\n",
    "X = df.drop(columns = 'Rating')\n",
    "y = pd.DataFrame(df.Rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:34.456812Z",
     "start_time": "2020-05-23T12:08:34.435574Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company \\n(Maker-if known)</th>\n",
       "      <th>Specific Bean Origin\\nor Bar Name</th>\n",
       "      <th>REF</th>\n",
       "      <th>Review\\nDate</th>\n",
       "      <th>Cocoa\\nPercent</th>\n",
       "      <th>Company\\nLocation</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Bean\\nType</th>\n",
       "      <th>Broad Bean\\nOrigin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Agua Grande</td>\n",
       "      <td>1876</td>\n",
       "      <td>2016</td>\n",
       "      <td>63%</td>\n",
       "      <td>France</td>\n",
       "      <td>3.75</td>\n",
       "      <td></td>\n",
       "      <td>Sao Tome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Kpime</td>\n",
       "      <td>1676</td>\n",
       "      <td>2015</td>\n",
       "      <td>70%</td>\n",
       "      <td>France</td>\n",
       "      <td>2.75</td>\n",
       "      <td></td>\n",
       "      <td>Togo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company \\n(Maker-if known) Specific Bean Origin\\nor Bar Name   REF  \\\n",
       "0                   A. Morin                       Agua Grande  1876   \n",
       "1                   A. Morin                             Kpime  1676   \n",
       "\n",
       "   Review\\nDate Cocoa\\nPercent Company\\nLocation  Rating Bean\\nType  \\\n",
       "0          2016            63%            France    3.75              \n",
       "1          2015            70%            France    2.75              \n",
       "\n",
       "  Broad Bean\\nOrigin  \n",
       "0           Sao Tome  \n",
       "1               Togo  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:34.709728Z",
     "start_time": "2020-05-23T12:08:34.706800Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_enc = ['Company\\nLocation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Encoding Modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:38.847271Z",
     "start_time": "2020-05-23T12:08:38.724141Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Target Encoder\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "import category_encoders.utils as util\n",
    "\n",
    "__author__ = 'chappers'\n",
    "\n",
    "\n",
    "class PercentileEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Target encoding for categorical features.\n",
    "\n",
    "    For the case of categorical target: features are replaced with a blend of posterior probability of the target\n",
    "    given particular categorical value and the prior probability of the target over all the training data.\n",
    "\n",
    "    For the case of continuous target: features are replaced with a blend of the expected value of the target\n",
    "    given particular categorical value and the expected value of the target over all the training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    verbose: int\n",
    "        integer indicating verbosity of the output. 0 for none.\n",
    "    cols: list\n",
    "        a list of columns to encode, if None, all string columns will be encoded.\n",
    "    drop_invariant: bool\n",
    "        boolean for whether or not to drop columns with 0 variance.\n",
    "    return_df: bool\n",
    "        boolean for whether to return a pandas DataFrame from transform (otherwise it will be a numpy array).\n",
    "    handle_missing: str\n",
    "        options are 'error', 'return_nan'  and 'value', defaults to 'value', which returns the target mean.\n",
    "    handle_unknown: str\n",
    "        options are 'error', 'return_nan' and 'value', defaults to 'value', which returns the target mean.\n",
    "    min_samples_leaf: int\n",
    "        minimum samples to take category average into account.\n",
    "    smoothing: float\n",
    "        smoothing effect to balance categorical average vs prior. Higher value means stronger regularization.\n",
    "        The value must be strictly bigger than 0.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> from category_encoders import *\n",
    "    >>> import pandas as pd\n",
    "    >>> from sklearn.datasets import load_boston\n",
    "    >>> bunch = load_boston()\n",
    "    >>> y = bunch.target\n",
    "    >>> X = pd.DataFrame(bunch.data, columns=bunch.feature_names)\n",
    "    >>> enc = TargetEncoder(cols=['CHAS', 'RAD']).fit(X, y)\n",
    "    >>> numeric_dataset = enc.transform(X)\n",
    "    >>> print(numeric_dataset.info())\n",
    "    <class 'pandas.core.frame.DataFrame'>\n",
    "    RangeIndex: 506 entries, 0 to 505\n",
    "    Data columns (total 13 columns):\n",
    "    CRIM       506 non-null float64\n",
    "    ZN         506 non-null float64\n",
    "    INDUS      506 non-null float64\n",
    "    CHAS       506 non-null float64\n",
    "    NOX        506 non-null float64\n",
    "    RM         506 non-null float64\n",
    "    AGE        506 non-null float64\n",
    "    DIS        506 non-null float64\n",
    "    RAD        506 non-null float64\n",
    "    TAX        506 non-null float64\n",
    "    PTRATIO    506 non-null float64\n",
    "    B          506 non-null float64\n",
    "    LSTAT      506 non-null float64\n",
    "    dtypes: float64(13)\n",
    "    memory usage: 51.5 KB\n",
    "    None\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    .. [1] A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems, from\n",
    "    https://dl.acm.org/citation.cfm?id=507538\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0, cols=None, drop_invariant=False, return_df=True, handle_missing='value',\n",
    "                     handle_unknown='value', min_samples_leaf=1, smoothing=1.0,percentile=50):\n",
    "        self.return_df = return_df\n",
    "        self.drop_invariant = drop_invariant\n",
    "        self.drop_cols = []\n",
    "        self.verbose = verbose\n",
    "        self.cols = cols\n",
    "        self.ordinal_encoder = None\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.smoothing = float(smoothing)  # Make smoothing a float so that python 2 does not treat as integer division\n",
    "        self._dim = None\n",
    "        self.mapping = None\n",
    "        self.handle_unknown = handle_unknown\n",
    "        self.handle_missing = handle_missing\n",
    "        self._mean = None\n",
    "        self.feature_names = None\n",
    "        self.percentile = percentile\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\"Fit encoder according to X and y.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : encoder\n",
    "            Returns self.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # unite the input into pandas types\n",
    "        X = util.convert_input(X)\n",
    "        y = util.convert_input_vector(y, X.index)\n",
    "\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"The length of X is \" + str(X.shape[0]) + \" but length of y is \" + str(y.shape[0]) + \".\")\n",
    "\n",
    "        self._dim = X.shape[1]\n",
    "\n",
    "        # if columns aren't passed, just use every string column\n",
    "        if self.cols is None:\n",
    "            self.cols = util.get_obj_cols(X)\n",
    "        else:\n",
    "            self.cols = util.convert_cols_to_list(self.cols)\n",
    "\n",
    "        if self.handle_missing == 'error':\n",
    "            if X[self.cols].isnull().any().any():\n",
    "                raise ValueError('Columns to be encoded can not contain null')\n",
    "\n",
    "        self.ordinal_encoder = OrdinalEncoder(\n",
    "            verbose=self.verbose,\n",
    "            cols=self.cols,\n",
    "            handle_unknown='value',\n",
    "            handle_missing='value'\n",
    "        )\n",
    "        self.ordinal_encoder = self.ordinal_encoder.fit(X)\n",
    "        X_ordinal = self.ordinal_encoder.transform(X)\n",
    "        self.mapping = self.fit_target_encoding(X_ordinal, y)\n",
    "        \n",
    "        X_temp = self.transform(X, override_return_df=True)\n",
    "        self.feature_names = list(X_temp.columns)\n",
    "\n",
    "        if self.drop_invariant:\n",
    "            self.drop_cols = []\n",
    "            X_temp = self.transform(X)\n",
    "            generated_cols = util.get_generated_cols(X, X_temp, self.cols)\n",
    "            self.drop_cols = [x for x in generated_cols if X_temp[x].var() <= 10e-5]\n",
    "            try:\n",
    "                [self.feature_names.remove(x) for x in self.drop_cols]\n",
    "            except KeyError as e:\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Could not remove column from feature names.\"\n",
    "                    \"Not found in generated cols.\\n{}\".format(e))\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def fit_target_encoding(self, X, y):\n",
    "        mapping = {}\n",
    "\n",
    "        for switch in self.ordinal_encoder.category_mapping:\n",
    "            col = switch.get('col')\n",
    "            values = switch.get('mapping')\n",
    "\n",
    "            prior = self._mean = y.mean()\n",
    "\n",
    "            stats = y.groupby(X[col]).apply(lambda x: np.percentile(x,self.percentile))\n",
    "\n",
    "            \n",
    "\n",
    "            mapping[col] = stats\n",
    "            #pdb.set_trace()\n",
    "        return mapping\n",
    "\n",
    "    def transform(self, X, y=None, override_return_df=False):\n",
    "        \"\"\"Perform the transformation to new categorical data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "        y : array-like, shape = [n_samples] when transform by leave one out\n",
    "            None, when transform without target info (such as transform test set)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        p : array, shape = [n_samples, n_numeric + N]\n",
    "            Transformed values with encoding applied.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if self.handle_missing == 'error':\n",
    "            if X[self.cols].isnull().any().any():\n",
    "                raise ValueError('Columns to be encoded can not contain null')\n",
    "\n",
    "        if self._dim is None:\n",
    "            raise ValueError('Must train encoder before it can be used to transform data.')\n",
    "\n",
    "        # unite the input into pandas types\n",
    "        X = util.convert_input(X)\n",
    "\n",
    "        # then make sure that it is the right size\n",
    "        if X.shape[1] != self._dim:\n",
    "            raise ValueError('Unexpected input dimension %d, expected %d' % (X.shape[1], self._dim,))\n",
    "\n",
    "        # if we are encoding the training data, we have to check the target\n",
    "        if y is not None:\n",
    "            y = util.convert_input_vector(y, X.index)\n",
    "            if X.shape[0] != y.shape[0]:\n",
    "                raise ValueError(\"The length of X is \" + str(X.shape[0]) + \" but length of y is \" + str(y.shape[0]) + \".\")\n",
    "\n",
    "        if not list(self.cols):\n",
    "            return X\n",
    "\n",
    "        X = self.ordinal_encoder.transform(X)\n",
    "\n",
    "        if self.handle_unknown == 'error':\n",
    "            if X[self.cols].isin([-1]).any().any():\n",
    "                raise ValueError('Unexpected categories found in dataframe')\n",
    "\n",
    "        X = self.target_encode(X)\n",
    "\n",
    "        if self.drop_invariant:\n",
    "            for col in self.drop_cols:\n",
    "                X.drop(col, 1, inplace=True)\n",
    "\n",
    "        if self.return_df or override_return_df:\n",
    "            return X\n",
    "        else:\n",
    "            return X.values\n",
    "\n",
    "\n",
    "    def target_encode(self, X_in):\n",
    "        X = X_in.copy(deep=True)\n",
    "\n",
    "        for col in self.cols:\n",
    "            X[col] = X[col].map(self.mapping[col])\n",
    "\n",
    "        return X\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"\n",
    "        Returns the names of all transformed / added columns.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        feature_names: list\n",
    "            A list with all feature names transformed or added.\n",
    "            Note: potentially dropped features are not included!\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(self.feature_names, list):\n",
    "            raise ValueError('Must fit data first. Affected feature names are not known before.')\n",
    "        else:\n",
    "            return self.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:58.902158Z",
     "start_time": "2020-05-23T12:08:58.899017Z"
    }
   },
   "outputs": [],
   "source": [
    "pe = PercentileEncoder(cols=cols_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:09:43.940277Z",
     "start_time": "2020-05-23T12:09:43.904984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PercentileEncoder(cols=['Company\\nLocation'], drop_invariant=False,\n",
       "                  handle_missing='value', handle_unknown='value',\n",
       "                  min_samples_leaf=1, percentile=50, return_df=True,\n",
       "                  smoothing=1.0, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:09:44.650769Z",
     "start_time": "2020-05-23T12:09:44.625868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company \\n(Maker-if known)</th>\n",
       "      <th>Specific Bean Origin\\nor Bar Name</th>\n",
       "      <th>REF</th>\n",
       "      <th>Review\\nDate</th>\n",
       "      <th>Cocoa\\nPercent</th>\n",
       "      <th>Company\\nLocation</th>\n",
       "      <th>Bean\\nType</th>\n",
       "      <th>Broad Bean\\nOrigin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Agua Grande</td>\n",
       "      <td>1876</td>\n",
       "      <td>2016</td>\n",
       "      <td>63%</td>\n",
       "      <td>3.25</td>\n",
       "      <td></td>\n",
       "      <td>Sao Tome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Kpime</td>\n",
       "      <td>1676</td>\n",
       "      <td>2015</td>\n",
       "      <td>70%</td>\n",
       "      <td>3.25</td>\n",
       "      <td></td>\n",
       "      <td>Togo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Atsane</td>\n",
       "      <td>1676</td>\n",
       "      <td>2015</td>\n",
       "      <td>70%</td>\n",
       "      <td>3.25</td>\n",
       "      <td></td>\n",
       "      <td>Togo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Akata</td>\n",
       "      <td>1680</td>\n",
       "      <td>2015</td>\n",
       "      <td>70%</td>\n",
       "      <td>3.25</td>\n",
       "      <td></td>\n",
       "      <td>Togo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Quilla</td>\n",
       "      <td>1704</td>\n",
       "      <td>2015</td>\n",
       "      <td>70%</td>\n",
       "      <td>3.25</td>\n",
       "      <td></td>\n",
       "      <td>Peru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>Zotter</td>\n",
       "      <td>Peru</td>\n",
       "      <td>647</td>\n",
       "      <td>2011</td>\n",
       "      <td>70%</td>\n",
       "      <td>3.25</td>\n",
       "      <td></td>\n",
       "      <td>Peru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>Zotter</td>\n",
       "      <td>Congo</td>\n",
       "      <td>749</td>\n",
       "      <td>2011</td>\n",
       "      <td>65%</td>\n",
       "      <td>3.25</td>\n",
       "      <td>Forastero</td>\n",
       "      <td>Congo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>Zotter</td>\n",
       "      <td>Kerala State</td>\n",
       "      <td>749</td>\n",
       "      <td>2011</td>\n",
       "      <td>65%</td>\n",
       "      <td>3.25</td>\n",
       "      <td>Forastero</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>Zotter</td>\n",
       "      <td>Kerala State</td>\n",
       "      <td>781</td>\n",
       "      <td>2011</td>\n",
       "      <td>62%</td>\n",
       "      <td>3.25</td>\n",
       "      <td></td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>Zotter</td>\n",
       "      <td>Brazil, Mitzi Blue</td>\n",
       "      <td>486</td>\n",
       "      <td>2010</td>\n",
       "      <td>65%</td>\n",
       "      <td>3.25</td>\n",
       "      <td></td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1795 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Company \\n(Maker-if known) Specific Bean Origin\\nor Bar Name   REF  \\\n",
       "0                      A. Morin                       Agua Grande  1876   \n",
       "1                      A. Morin                             Kpime  1676   \n",
       "2                      A. Morin                            Atsane  1676   \n",
       "3                      A. Morin                             Akata  1680   \n",
       "4                      A. Morin                            Quilla  1704   \n",
       "...                         ...                               ...   ...   \n",
       "1790                     Zotter                              Peru   647   \n",
       "1791                     Zotter                             Congo   749   \n",
       "1792                     Zotter                      Kerala State   749   \n",
       "1793                     Zotter                      Kerala State   781   \n",
       "1794                     Zotter                Brazil, Mitzi Blue   486   \n",
       "\n",
       "      Review\\nDate Cocoa\\nPercent  Company\\nLocation Bean\\nType  \\\n",
       "0             2016            63%               3.25              \n",
       "1             2015            70%               3.25              \n",
       "2             2015            70%               3.25              \n",
       "3             2015            70%               3.25              \n",
       "4             2015            70%               3.25              \n",
       "...            ...            ...                ...        ...   \n",
       "1790          2011            70%               3.25              \n",
       "1791          2011            65%               3.25  Forastero   \n",
       "1792          2011            65%               3.25  Forastero   \n",
       "1793          2011            62%               3.25              \n",
       "1794          2010            65%               3.25              \n",
       "\n",
       "     Broad Bean\\nOrigin  \n",
       "0              Sao Tome  \n",
       "1                  Togo  \n",
       "2                  Togo  \n",
       "3                  Togo  \n",
       "4                  Peru  \n",
       "...                 ...  \n",
       "1790               Peru  \n",
       "1791              Congo  \n",
       "1792              India  \n",
       "1793              India  \n",
       "1794             Brazil  \n",
       "\n",
       "[1795 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:59.751326Z",
     "start_time": "2020-05-23T12:08:59.721448Z"
    }
   },
   "outputs": [],
   "source": [
    "pe.fit_target_encoding(X[cols_enc],y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:09:00.342883Z",
     "start_time": "2020-05-23T12:09:00.319097Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(cols_enc)['Rating'].apply(lambda x: np.percentile(x,50));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:09:11.577628Z",
     "start_time": "2020-05-23T12:09:11.569516Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tr,X_te,y_tr,y_te = train_test_split(df.drop(columns = 'Rating'),df.Rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:09:12.106285Z",
     "start_time": "2020-05-23T12:09:12.102116Z"
    }
   },
   "outputs": [],
   "source": [
    "pe = PercentileEncoder(cols=cols_enc)\n",
    "lasso = linear_model.Lasso()\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:12:40.967828Z",
     "start_time": "2020-05-23T12:12:40.962995Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    \n",
    "    ('pe',pe),\n",
    "    ('selector', TypeSelector(np.number)), # Selects Numerical Columns only\n",
    "    ('scaler',scaler),\n",
    "    ('lasso',lasso)])\n",
    "\n",
    "pipeline_grid = {\n",
    "    #\"lasso__alpha\":[0.000001,0.001,0.1,1],\n",
    "    #\"pe__percentile\":[10,50,80]\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:12:41.830023Z",
     "start_time": "2020-05-23T12:12:41.825719Z"
    }
   },
   "outputs": [],
   "source": [
    "n_jobs=-1\n",
    "cv = 3\n",
    "pipe_cv = GridSearchCV(pipe, param_grid=pipeline_grid, n_jobs = n_jobs, cv=cv,scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:12:42.373735Z",
     "start_time": "2020-05-23T12:12:42.053735Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/cmougan/anaconda3/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"/Users/cmougan/anaconda3/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/Users/cmougan/anaconda3/lib/python3.6/site-packages/joblib/_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/cmougan/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"/Users/cmougan/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/Users/cmougan/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 556, in _fit_and_score\n    test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)\n  File \"/Users/cmougan/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 599, in _score\n    return _multimetric_score(estimator, X_test, y_test, scorer)\n  File \"/Users/cmougan/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 629, in _multimetric_score\n    score = scorer(estimator, X_test, y_test)\n  File \"/Users/cmougan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/scorer.py\", line 90, in __call__\n    y_pred = estimator.predict(X)\n  File \"/Users/cmougan/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\", line 116, in <lambda>\n    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n  File \"/Users/cmougan/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\", line 422, in predict\n    return self.steps[-1][-1].predict(Xt, **predict_params)\n  File \"/Users/cmougan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\", line 221, in predict\n    return self._decision_function(X)\n  File \"/Users/cmougan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py\", line 795, in _decision_function\n    return super()._decision_function(X)\n  File \"/Users/cmougan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\", line 204, in _decision_function\n    X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n  File \"/Users/cmougan/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 542, in check_array\n    allow_nan=force_all_finite == 'allow-nan')\n  File \"/Users/cmougan/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 56, in _assert_all_finite\n    raise ValueError(msg_err.format(type_err, X.dtype))\nValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-716308803a37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipe_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 667\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "pipe_cv.fit(X_tr,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:12:42.470782Z",
     "start_time": "2020-05-23T12:12:42.460932Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'cv_results_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-4bd7f43ffa45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'cv_results_'"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(pipe_cv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# My own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.880510Z",
     "start_time": "2020-05-23T12:08:15.790Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import category_encoders.utils as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.881414Z",
     "start_time": "2020-05-23T12:08:15.792Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = util.convert_input(X)\n",
    "y = util.convert_input_vector(y, X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.882900Z",
     "start_time": "2020-05-23T12:08:15.794Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y.groupby(X[cols_enc]).agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.883734Z",
     "start_time": "2020-05-23T12:08:15.797Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.groupby(cols_enc)['Rating'].apply(lambda x: np.percentile(x,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.884719Z",
     "start_time": "2020-05-23T12:08:15.800Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.885468Z",
     "start_time": "2020-05-23T12:08:15.801Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.percentile(a,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.886968Z",
     "start_time": "2020-05-23T12:08:15.803Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Percentile_Encoding(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    This class implements fit and transform methods that allows to encode categorical features in different ways.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,columns=\"All\",return_categorical=True,percentile= 50):\n",
    "        #cols: list -> a list of columns to encode, if All, all string columns will be encoded.\n",
    "        \n",
    "        #self._allowed_encodings = [\"TargetEncoder\",\"WOEEncoder\",\"CatBoostEncoder\",\"OneHotEncoder\"]           \n",
    "        #assert encoding_type in self._allowed_encodings, \"the encoding type introduced {} is not valid. Please use one in {}\".format(encoding_type, self._allowed_encodings)\n",
    "        #self.encoding_type = encoding_type\n",
    "        \n",
    "        self.columns = columns\n",
    "        self.return_categorical = return_categorical\n",
    "        self.percentile = percentile\n",
    "        \n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \"\"\"\n",
    "        This method learns encodings for categorical variables/values.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Obtain a list of categorical variables\n",
    "        if self.columns == \"All\":\n",
    "            self.categorical_cols = X.columns[X.dtypes==object].tolist() +  X.columns[X.dtypes==\"category\"].tolist()\n",
    "        else:\n",
    "            self.categorical_cols = self.columns\n",
    "        \n",
    "    \n",
    "        # Split the data into categorical and numerical\n",
    "        self.data_encode = X[self.categorical_cols]\n",
    "\n",
    "        # Fit the encoder\n",
    "        self.mapping = self.fit_percentil_encoding(self.data_encode, y)\n",
    "        return self\n",
    "    \n",
    "    def fit_percentil_encoding(self, X, y):\n",
    "        mapping = {}\n",
    "\n",
    "        for col in X.columns:\n",
    "            \n",
    "            stats = X.groupby(col)[y.Rating].apply(lambda x: np.percentile(x,self.percentile))\n",
    "            \n",
    "            mapping[col] = stats\n",
    "        return mapping\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        \n",
    "        if self.columns == \"All\":\n",
    "            self.categorical_cols = X.columns[X.dtypes==object].tolist() +  X.columns[X.dtypes==\"category\"].tolist()\n",
    "        else:\n",
    "            self.categorical_cols = self.columns\n",
    "        \n",
    "       \n",
    "    \n",
    "        # Split the data into categorical and numerical\n",
    "        self.data_encode = X[self.categorical_cols]\n",
    "        \n",
    "        # Transform the data\n",
    "        self.transformed = self.enc.transform(self.data_encode)\n",
    "        \n",
    "        # Modify the names of the columns with the proper suffix\n",
    "        self.new_names = []\n",
    "        for c in self.transformed.columns:\n",
    "            self.new_names.append(c+'_'+self.encoding_type)\n",
    "        self.transformed.columns = self.new_names\n",
    "         \n",
    "        if self.return_categorical:\n",
    "            #print('The encoding {} has made {} columns, the input was {} and the output shape{}'.\n",
    "             #     format(self.encoding_type,self.transformed.shape, X.shape,self.transformed.join(X).shape))\n",
    "            #print(self.transformed.join(X).dtypes)\n",
    "\n",
    "            return self.transformed.join(X)\n",
    "        else:\n",
    "            return self.transformed.join(X)._get_numeric_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.887678Z",
     "start_time": "2020-05-23T12:08:15.804Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pe = Percentile_Encoding(columns=cols_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.888464Z",
     "start_time": "2020-05-23T12:08:15.806Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pe.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.889466Z",
     "start_time": "2020-05-23T12:08:15.808Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Target Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T11:27:51.632198Z",
     "start_time": "2020-05-23T11:27:51.429188Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.890418Z",
     "start_time": "2020-05-23T12:08:15.820Z"
    }
   },
   "outputs": [],
   "source": [
    "te = TargetEncoder(cols=cols_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.891375Z",
     "start_time": "2020-05-23T12:08:15.822Z"
    }
   },
   "outputs": [],
   "source": [
    "te.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.892246Z",
     "start_time": "2020-05-23T12:08:15.824Z"
    }
   },
   "outputs": [],
   "source": [
    "te.fit_target_encoding(X[cols_enc],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.893233Z",
     "start_time": "2020-05-23T12:08:15.826Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(cols_enc)['Rating'].apply(lambda x: np.percentile(x,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Target Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.894406Z",
     "start_time": "2020-05-23T12:08:15.831Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Target Encoder\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "import category_encoders.utils as util\n",
    "\n",
    "__author__ = 'chappers'\n",
    "\n",
    "\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Target encoding for categorical features.\n",
    "\n",
    "    For the case of categorical target: features are replaced with a blend of posterior probability of the target\n",
    "    given particular categorical value and the prior probability of the target over all the training data.\n",
    "\n",
    "    For the case of continuous target: features are replaced with a blend of the expected value of the target\n",
    "    given particular categorical value and the expected value of the target over all the training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    verbose: int\n",
    "        integer indicating verbosity of the output. 0 for none.\n",
    "    cols: list\n",
    "        a list of columns to encode, if None, all string columns will be encoded.\n",
    "    drop_invariant: bool\n",
    "        boolean for whether or not to drop columns with 0 variance.\n",
    "    return_df: bool\n",
    "        boolean for whether to return a pandas DataFrame from transform (otherwise it will be a numpy array).\n",
    "    handle_missing: str\n",
    "        options are 'error', 'return_nan'  and 'value', defaults to 'value', which returns the target mean.\n",
    "    handle_unknown: str\n",
    "        options are 'error', 'return_nan' and 'value', defaults to 'value', which returns the target mean.\n",
    "    min_samples_leaf: int\n",
    "        minimum samples to take category average into account.\n",
    "    smoothing: float\n",
    "        smoothing effect to balance categorical average vs prior. Higher value means stronger regularization.\n",
    "        The value must be strictly bigger than 0.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> from category_encoders import *\n",
    "    >>> import pandas as pd\n",
    "    >>> from sklearn.datasets import load_boston\n",
    "    >>> bunch = load_boston()\n",
    "    >>> y = bunch.target\n",
    "    >>> X = pd.DataFrame(bunch.data, columns=bunch.feature_names)\n",
    "    >>> enc = TargetEncoder(cols=['CHAS', 'RAD']).fit(X, y)\n",
    "    >>> numeric_dataset = enc.transform(X)\n",
    "    >>> print(numeric_dataset.info())\n",
    "    <class 'pandas.core.frame.DataFrame'>\n",
    "    RangeIndex: 506 entries, 0 to 505\n",
    "    Data columns (total 13 columns):\n",
    "    CRIM       506 non-null float64\n",
    "    ZN         506 non-null float64\n",
    "    INDUS      506 non-null float64\n",
    "    CHAS       506 non-null float64\n",
    "    NOX        506 non-null float64\n",
    "    RM         506 non-null float64\n",
    "    AGE        506 non-null float64\n",
    "    DIS        506 non-null float64\n",
    "    RAD        506 non-null float64\n",
    "    TAX        506 non-null float64\n",
    "    PTRATIO    506 non-null float64\n",
    "    B          506 non-null float64\n",
    "    LSTAT      506 non-null float64\n",
    "    dtypes: float64(13)\n",
    "    memory usage: 51.5 KB\n",
    "    None\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    .. [1] A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems, from\n",
    "    https://dl.acm.org/citation.cfm?id=507538\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0, cols=None, drop_invariant=False, return_df=True, handle_missing='value',\n",
    "                     handle_unknown='value', min_samples_leaf=1, smoothing=1.0):\n",
    "        self.return_df = return_df\n",
    "        self.drop_invariant = drop_invariant\n",
    "        self.drop_cols = []\n",
    "        self.verbose = verbose\n",
    "        self.cols = cols\n",
    "        self.ordinal_encoder = None\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.smoothing = float(smoothing)  # Make smoothing a float so that python 2 does not treat as integer division\n",
    "        self._dim = None\n",
    "        self.mapping = None\n",
    "        self.handle_unknown = handle_unknown\n",
    "        self.handle_missing = handle_missing\n",
    "        self._mean = None\n",
    "        self.feature_names = None\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\"Fit encoder according to X and y.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : encoder\n",
    "            Returns self.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # unite the input into pandas types\n",
    "        X = util.convert_input(X)\n",
    "        y = util.convert_input_vector(y, X.index)\n",
    "\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"The length of X is \" + str(X.shape[0]) + \" but length of y is \" + str(y.shape[0]) + \".\")\n",
    "\n",
    "        self._dim = X.shape[1]\n",
    "\n",
    "        # if columns aren't passed, just use every string column\n",
    "        if self.cols is None:\n",
    "            self.cols = util.get_obj_cols(X)\n",
    "        else:\n",
    "            self.cols = util.convert_cols_to_list(self.cols)\n",
    "\n",
    "        if self.handle_missing == 'error':\n",
    "            if X[self.cols].isnull().any().any():\n",
    "                raise ValueError('Columns to be encoded can not contain null')\n",
    "\n",
    "        self.ordinal_encoder = OrdinalEncoder(\n",
    "            verbose=self.verbose,\n",
    "            cols=self.cols,\n",
    "            handle_unknown='value',\n",
    "            handle_missing='value'\n",
    "        )\n",
    "        self.ordinal_encoder = self.ordinal_encoder.fit(X)\n",
    "        X_ordinal = self.ordinal_encoder.transform(X)\n",
    "        self.mapping = self.fit_target_encoding(X_ordinal, y)\n",
    "        \n",
    "        X_temp = self.transform(X, override_return_df=True)\n",
    "        self.feature_names = list(X_temp.columns)\n",
    "\n",
    "        if self.drop_invariant:\n",
    "            self.drop_cols = []\n",
    "            X_temp = self.transform(X)\n",
    "            generated_cols = util.get_generated_cols(X, X_temp, self.cols)\n",
    "            self.drop_cols = [x for x in generated_cols if X_temp[x].var() <= 10e-5]\n",
    "            try:\n",
    "                [self.feature_names.remove(x) for x in self.drop_cols]\n",
    "            except KeyError as e:\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Could not remove column from feature names.\"\n",
    "                    \"Not found in generated cols.\\n{}\".format(e))\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def fit_target_encoding(self, X, y):\n",
    "        mapping = {}\n",
    "\n",
    "        for switch in self.ordinal_encoder.category_mapping:\n",
    "            col = switch.get('col')\n",
    "            values = switch.get('mapping')\n",
    "\n",
    "            prior = self._mean = y.mean()\n",
    "\n",
    "            stats = y.groupby(X[col]).agg(['count', 'mean'])\n",
    "\n",
    "            smoove = 1 / (1 + np.exp(-(stats['count'] - self.min_samples_leaf) / self.smoothing))\n",
    "            smoothing = prior * (1 - smoove) + stats['mean'] * smoove\n",
    "            smoothing[stats['count'] == 1] = prior\n",
    "\n",
    "            if self.handle_unknown == 'return_nan':\n",
    "                smoothing.loc[-1] = np.nan\n",
    "            elif self.handle_unknown == 'value':\n",
    "                smoothing.loc[-1] = prior\n",
    "\n",
    "            if self.handle_missing == 'return_nan':\n",
    "                smoothing.loc[values.loc[np.nan]] = np.nan\n",
    "            elif self.handle_missing == 'value':\n",
    "                smoothing.loc[-2] = prior\n",
    "\n",
    "            mapping[col] = smoothing\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    def transform(self, X, y=None, override_return_df=False):\n",
    "        \"\"\"Perform the transformation to new categorical data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "        y : array-like, shape = [n_samples] when transform by leave one out\n",
    "            None, when transform without target info (such as transform test set)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        p : array, shape = [n_samples, n_numeric + N]\n",
    "            Transformed values with encoding applied.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if self.handle_missing == 'error':\n",
    "            if X[self.cols].isnull().any().any():\n",
    "                raise ValueError('Columns to be encoded can not contain null')\n",
    "\n",
    "        if self._dim is None:\n",
    "            raise ValueError('Must train encoder before it can be used to transform data.')\n",
    "\n",
    "        # unite the input into pandas types\n",
    "        X = util.convert_input(X)\n",
    "\n",
    "        # then make sure that it is the right size\n",
    "        if X.shape[1] != self._dim:\n",
    "            raise ValueError('Unexpected input dimension %d, expected %d' % (X.shape[1], self._dim,))\n",
    "\n",
    "        # if we are encoding the training data, we have to check the target\n",
    "        if y is not None:\n",
    "            y = util.convert_input_vector(y, X.index)\n",
    "            if X.shape[0] != y.shape[0]:\n",
    "                raise ValueError(\"The length of X is \" + str(X.shape[0]) + \" but length of y is \" + str(y.shape[0]) + \".\")\n",
    "\n",
    "        if not list(self.cols):\n",
    "            return X\n",
    "\n",
    "        X = self.ordinal_encoder.transform(X)\n",
    "\n",
    "        if self.handle_unknown == 'error':\n",
    "            if X[self.cols].isin([-1]).any().any():\n",
    "                raise ValueError('Unexpected categories found in dataframe')\n",
    "\n",
    "        X = self.target_encode(X)\n",
    "\n",
    "        if self.drop_invariant:\n",
    "            for col in self.drop_cols:\n",
    "                X.drop(col, 1, inplace=True)\n",
    "\n",
    "        if self.return_df or override_return_df:\n",
    "            return X\n",
    "        else:\n",
    "            return X.values\n",
    "\n",
    "\n",
    "    def target_encode(self, X_in):\n",
    "        X = X_in.copy(deep=True)\n",
    "\n",
    "        for col in self.cols:\n",
    "            X[col] = X[col].map(self.mapping[col])\n",
    "\n",
    "        return X\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"\n",
    "        Returns the names of all transformed / added columns.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        feature_names: list\n",
    "            A list with all feature names transformed or added.\n",
    "            Note: potentially dropped features are not included!\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(self.feature_names, list):\n",
    "            raise ValueError('Must fit data first. Affected feature names are not known before.')\n",
    "        else:\n",
    "            return self.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.895452Z",
     "start_time": "2020-05-23T12:08:15.834Z"
    }
   },
   "outputs": [],
   "source": [
    "te = TargetEncoder(cols=cols_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.896418Z",
     "start_time": "2020-05-23T12:08:15.836Z"
    }
   },
   "outputs": [],
   "source": [
    "te.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:08:15.897200Z",
     "start_time": "2020-05-23T12:08:15.838Z"
    }
   },
   "outputs": [],
   "source": [
    "te.transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
